{
  "paragraphs": [
    {
      "text": "%md\n\n# Introduction to Spark \u0026 Zeppelin\n#### An overview of the two Big Data tools.\n\nBefore pursuing, make sure you have saved the previous Interpreter binding window, otherwise you will get an error like `paragraph not found` when executing cells",
      "dateUpdated": "Feb 25, 2017 9:41:19 PM",
      "config": {
        "enabled": true,
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555170_-614444159",
      "id": "20161205-081916_1418679304",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch1\u003eIntroduction to Spark \u0026amp; Zeppelin\u003c/h1\u003e\n\u003ch4\u003eAn overview of the two Big Data tools.\u003c/h4\u003e\n\u003cp\u003eBefore pursuing, make sure you have saved the previous Interpreter binding window, otherwise you will get an error like \u003ccode\u003eparagraph not found\u003c/code\u003e when executing cells\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:19 PM",
      "dateFinished": "Feb 25, 2017 9:41:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n![Spark logo](http://spark.apache.org/images/spark-logo-trademark.png)\n#\n[Apache Spark](http://spark.apache.org/) is a cluster computing engine designed to be __fast__ and __general-purpose__, making it the ideal choice for processing of large datasets. It answers those two points with __efficient data sharing__ accross computations.\n\u003chr/\u003e\nThe past years have seen a major changes in computing systems, as growing data volumes required more and more applications to scale out to large clusters. To solve this problem, a wide range of new programming models have been designed to manage multiple types of computations in a distributed fashion, without having people learn too much about distributed systems. Those programming models would need to deal with _parallelism, fault-tolerance and resource sharing_ for us.\n\n[Google\u0027s MapReduce](https://en.wikipedia.org/wiki/MapReduce) presented a simple and general model for batch processing, which handles faults and parallelism easily. Unfortunately the programming model is not adapted for other types of workloads, and multiple specialized systems were born to answer a specific need in a distributed way. \n* Iterative : Giraph\n* Interactive : Impala, Piccolo, Greenplum\n* Streaming : Storm, Millwheel\n\n#\nThe initial goal of Apache Spark is to try and unify all of the workloads for generality purposes. [Matei Zaharia](https://cs.stanford.edu/~matei/) in his [PhD dissertation](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf) suggests that most of the data flow models that required a specialized system needed _efficient data sharing_ accross computations:\n* Iterative algorithms like PageRank or K-Means need to make multiple passes over the same dataset\n* Interactive data mining often requires running multiple ad-hoc queries on the same subset of data\n* Streaming applications need to maintain and share state over time.\n\nHe then proposes to create a new abstraction that gives its users direct control over data sharing, something that other specialized systems would have built-in for their specific needs. The abstraction is implemented inside a new engine that is today called Apache Spark. The engine makes it possible to support more types of computations than with the original MapReduce in a more efficient way, including interactive queries and stream processing. ",
      "dateUpdated": "Feb 25, 2017 9:41:19 PM",
      "config": {
        "enabled": true,
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555171_-614828908",
      "id": "20161205-125129_704251374",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003e\u003cimg src\u003d\"http://spark.apache.org/images/spark-logo-trademark.png\" alt\u003d\"Spark logo\" /\u003e\u003c/p\u003e\n\u003ch1\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca href\u003d\"http://spark.apache.org/\"\u003eApache Spark\u003c/a\u003e is a cluster computing engine designed to be \u003cstrong\u003efast\u003c/strong\u003e and \u003cstrong\u003egeneral-purpose\u003c/strong\u003e, making it the ideal choice for processing of large datasets. It answers those two points with \u003cstrong\u003eefficient data sharing\u003c/strong\u003e accross computations.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp\u003eThe past years have seen a major changes in computing systems, as growing data volumes required more and more applications to scale out to large clusters. To solve this problem, a wide range of new programming models have been designed to manage multiple types of computations in a distributed fashion, without having people learn too much about distributed systems. Those programming models would need to deal with \u003cem\u003eparallelism, fault-tolerance and resource sharing\u003c/em\u003e for us.\u003c/p\u003e\n\u003cp\u003e\u003ca href\u003d\"https://en.wikipedia.org/wiki/MapReduce\"\u003eGoogle\u0027s MapReduce\u003c/a\u003e presented a simple and general model for batch processing, which handles faults and parallelism easily. Unfortunately the programming model is not adapted for other types of workloads, and multiple specialized systems were born to answer a specific need in a distributed way.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIterative : Giraph\u003c/li\u003e\n\u003cli\u003eInteractive : Impala, Piccolo, Greenplum\u003c/li\u003e\n\u003cli\u003eStreaming : Storm, Millwheel\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1\u003e\u003c/h1\u003e\n\u003cp\u003eThe initial goal of Apache Spark is to try and unify all of the workloads for generality purposes. \u003ca href\u003d\"https://cs.stanford.edu/~matei/\"\u003eMatei Zaharia\u003c/a\u003e in his \u003ca href\u003d\"https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf\"\u003ePhD dissertation\u003c/a\u003e suggests that most of the data flow models that required a specialized system needed \u003cem\u003eefficient data sharing\u003c/em\u003e accross computations:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eIterative algorithms like PageRank or K-Means need to make multiple passes over the same dataset\u003c/li\u003e\n\u003cli\u003eInteractive data mining often requires running multiple ad-hoc queries on the same subset of data\u003c/li\u003e\n\u003cli\u003eStreaming applications need to maintain and share state over time.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eHe then proposes to create a new abstraction that gives its users direct control over data sharing, something that other specialized systems would have built-in for their specific needs. The abstraction is implemented inside a new engine that is today called Apache Spark. The engine makes it possible to support more types of computations than with the original MapReduce in a more efficient way, including interactive queries and stream processing.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:22 PM",
      "dateFinished": "Feb 25, 2017 9:41:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n![](https://zeppelin.apache.org/assets/themes/zeppelin/img/zeppelin_classic_logo.png)\n#\n[Apache Zeppelin](https://zeppelin.apache.org/) is a web-based notebook that enables interactive data analytics. It is  famous for its very strong Apache Spark integration.\n\u003chr/\u003e\n\nApache Zeppelin\u0027s purpose is to provide engineers and scientists with an interface for all Big Data needs, which comes bundled with the means for analyzing, collaborating and sharing data on top of common Big Data frameworks.\n\nThe Apache Zeppelin interpreter concept allows the plugin of a language or data-processing backend into Zeppelin _(think of it like a kernel for Jupyter notebook)_. Currently it supports [many interpreters](https://zeppelin.apache.org/docs/0.6.2/manual/interpreterinstallation.html) like Apache Spark, Python, JDBC, Markdown and shell. Creation of a custom interpreter is also possible by extending the necessary abstract class.\n\nYou can view examples of Zeppelin notebooks [here](https://www.zeppelinhub.com/viewer).\n\nToday, Apache Zeppelin comes bundled with most Big Data distributions _(Cloudera, Hortonworks...)_ as the main tool for interactive Big Data analytics, which makes for a good reason to have you try Apache Spark interactively in a Zeppelin notebook.",
      "dateUpdated": "Feb 25, 2017 9:41:19 PM",
      "config": {
        "enabled": true,
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555172_-616752652",
      "id": "20161205-125209_891936396",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003e\u003cimg src\u003d\"https://zeppelin.apache.org/assets/themes/zeppelin/img/zeppelin_classic_logo.png\" alt\u003d\"\" /\u003e\u003c/p\u003e\n\u003ch1\u003e\u003c/h1\u003e\n\u003cp\u003e\u003ca href\u003d\"https://zeppelin.apache.org/\"\u003eApache Zeppelin\u003c/a\u003e is a web-based notebook that enables interactive data analytics. It is  famous for its very strong Apache Spark integration.\u003c/p\u003e\n\u003chr/\u003e\n\u003cp\u003eApache Zeppelin\u0027s purpose is to provide engineers and scientists with an interface for all Big Data needs, which comes bundled with the means for analyzing, collaborating and sharing data on top of common Big Data frameworks.\u003c/p\u003e\n\u003cp\u003eThe Apache Zeppelin interpreter concept allows the plugin of a language or data-processing backend into Zeppelin \u003cem\u003e(think of it like a kernel for Jupyter notebook)\u003c/em\u003e. Currently it supports \u003ca href\u003d\"https://zeppelin.apache.org/docs/0.6.2/manual/interpreterinstallation.html\"\u003emany interpreters\u003c/a\u003e like Apache Spark, Python, JDBC, Markdown and shell. Creation of a custom interpreter is also possible by extending the necessary abstract class.\u003c/p\u003e\n\u003cp\u003eYou can view examples of Zeppelin notebooks \u003ca href\u003d\"https://www.zeppelinhub.com/viewer\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eToday, Apache Zeppelin comes bundled with most Big Data distributions \u003cem\u003e(Cloudera, Hortonworks\u0026hellip;)\u003c/em\u003e as the main tool for interactive Big Data analytics, which makes for a good reason to have you try Apache Spark interactively in a Zeppelin notebook.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:22 PM",
      "dateFinished": "Feb 25, 2017 9:41:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n###Interpreter\n\nApache Zeppelin is primarily a notebook. A Zeppelin notebook consists of a set of cells with an interpreter _(or language backend)_ attached.\n\nFor example, this cell is linked to the Markdown interpreter, so when you run the cell it parses the code as Markdown and then displays the result.\n\nYou will find the most useful markdown components [on this cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet).\n\nIn each cell, there is a toolbar with four items: \n* The `Play` button (blue triangle) lets you run the paragraph. You can also press `Shift + Enter` to run the cell when selected\n* The `Show/Hide editor` if you want to see the source code for the cell\n* The `Show/Hide output` if you want to see the output for the cell\n* The `Settings` item, for more advanced options like adding/managing cells.\n\n\u003chr/\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\nNow you can press `Show editor` to see the source code of the cell. You will see that it starts with `%md`, which specifies to the cell that it should use the Markdown interpreter. You can edit the text and then run the cell to see the editing in the output.",
      "dateUpdated": "Feb 25, 2017 9:41:20 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555172_-616752652",
      "id": "20161205-134434_158643588",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eInterpreter\u003c/h3\u003e\n\u003cp\u003eApache Zeppelin is primarily a notebook. A Zeppelin notebook consists of a set of cells with an interpreter \u003cem\u003e(or language backend)\u003c/em\u003e attached.\u003c/p\u003e\n\u003cp\u003eFor example, this cell is linked to the Markdown interpreter, so when you run the cell it parses the code as Markdown and then displays the result.\u003c/p\u003e\n\u003cp\u003eYou will find the most useful markdown components \u003ca href\u003d\"https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\"\u003eon this cheatsheet\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIn each cell, there is a toolbar with four items:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe \u003ccode\u003ePlay\u003c/code\u003e button (blue triangle) lets you run the paragraph. You can also press \u003ccode\u003eShift + Enter\u003c/code\u003e to run the cell when selected\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003eShow/Hide editor\u003c/code\u003e if you want to see the source code for the cell\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003eShow/Hide output\u003c/code\u003e if you want to see the output for the cell\u003c/li\u003e\n\u003cli\u003eThe \u003ccode\u003eSettings\u003c/code\u003e item, for more advanced options like adding/managing cells.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr/\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\n\u003cp\u003eNow you can press \u003ccode\u003eShow editor\u003c/code\u003e to see the source code of the cell. You will see that it starts with \u003ccode\u003e%md\u003c/code\u003e, which specifies to the cell that it should use the Markdown interpreter. You can edit the text and then run the cell to see the editing in the output.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:23 PM",
      "dateFinished": "Feb 25, 2017 9:41:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nApache Zeppelin follows the responsive 12-column grid system. In the following row we show 3 different interpreters in action. \n\nYou will find the width of the cell inside the settings button.\n\u003chr/\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\nYou can go on and try to modify the source code in the three following cells and re-execute the code. For example, try to type `1+2` in the Python cell. TEST.",
      "dateUpdated": "Feb 25, 2017 9:41:20 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555182_-619061146",
      "id": "20161205-140658_147950685",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eApache Zeppelin follows the responsive 12-column grid system. In the following row we show 3 different interpreters in action.\u003c/p\u003e\n\u003cp\u003eYou will find the width of the cell inside the settings button.\u003c/p\u003e\n\u003chr/\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\n\u003cp\u003eYou can go on and try to modify the source code in the three following cells and re-execute the code. For example, try to type \u003ccode\u003e1+2\u003c/code\u003e in the Python cell. TEST.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:24 PM",
      "dateFinished": "Feb 25, 2017 9:41:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Shell cell",
      "text": "%sh\necho \"Hello world\"",
      "dateUpdated": "Feb 25, 2017 9:41:20 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "editorMode": "ace/mode/sh",
        "editorHide": false,
        "colWidth": 4.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555190_-609827172",
      "id": "20161205-135504_61539619",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "Hello world\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:21 PM",
      "dateFinished": "Feb 25, 2017 9:41:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Python cell",
      "text": "%python\nprint(1+2)",
      "dateUpdated": "Feb 25, 2017 9:47:46 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "editorMode": "ace/mode/python",
        "colWidth": 4.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555191_-610211921",
      "id": "20161205-140620_27821049",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "3\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:47:47 PM",
      "dateFinished": "Feb 25, 2017 9:47:47 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Spark/Scala cell",
      "text": "println(\"Hello world\")",
      "dateUpdated": "Feb 25, 2017 9:41:21 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 4.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555191_-610211921",
      "id": "20161205-140627_1063236175",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "Hello world\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:22 PM",
      "dateFinished": "Feb 25, 2017 9:42:21 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nAs you can see, we have not chosen an interpreter in the last column. \n\nA default interpreter is set at the beginning of the notebook, on the right-hand of the menu toolbar at the top in the menu `Interpreter binding` and represented by a gear. You can also select/deselect usable interpreters for the notebook.\n\nFor more information on how each interpreter is configured, you can check the interpreter menu, in the dropdown menu with your username _(anonymous for now. Do note that Apache Zeppelin also support user authentication for multitenancy but we are not going to delve into that)_.\n\nFor now, we assume that the default interpreter is the Apache Spark/Scala interpreter, so the default cell parses Spark/Scala code. The tutorial series will be mostly using this interpreter. No Scala knowledge is needed to pursue through the series, as we will only use basic functional programming concepts.\n\n_If you are wondering why Scala, Apache Spark is implemented in Scala, and so is its main API. Apache Spark draws from the functional paradigm for representing its computations the same way as MapReduce does. A common justification is Spark and MapReduce specialize in coarse-grained transformations that apply the same operation on many data items, which is a good fit for parallel applications, and the functional programming paradigm fits the idea._",
      "dateUpdated": "Feb 25, 2017 9:41:22 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555191_-610211921",
      "id": "20161205-140636_1331288934",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eAs you can see, we have not chosen an interpreter in the last column.\u003c/p\u003e\n\u003cp\u003eA default interpreter is set at the beginning of the notebook, on the right-hand of the menu toolbar at the top in the menu \u003ccode\u003eInterpreter binding\u003c/code\u003e and represented by a gear. You can also select/deselect usable interpreters for the notebook.\u003c/p\u003e\n\u003cp\u003eFor more information on how each interpreter is configured, you can check the interpreter menu, in the dropdown menu with your username \u003cem\u003e(anonymous for now. Do note that Apache Zeppelin also support user authentication for multitenancy but we are not going to delve into that)\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eFor now, we assume that the default interpreter is the Apache Spark/Scala interpreter, so the default cell parses Spark/Scala code. The tutorial series will be mostly using this interpreter. No Scala knowledge is needed to pursue through the series, as we will only use basic functional programming concepts.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003eIf you are wondering why Scala, Apache Spark is implemented in Scala, and so is its main API. Apache Spark draws from the functional paradigm for representing its computations the same way as MapReduce does. A common justification is Spark and MapReduce specialize in coarse-grained transformations that apply the same operation on many data items, which is a good fit for parallel applications, and the functional programming paradigm fits the idea.\u003c/em\u003e\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:24 PM",
      "dateFinished": "Feb 25, 2017 9:41:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### The Spark/Scala interpreter\n\nLet\u0027s dive into the Spark/Scala interpreter now. First, we need to describe the Scala language a bit more.\n\nScala is a functional object-oriented language.\n\nThe following row shows some commands in Scala. Do note that you can use autocompletion with `Ctrl + .`\n\n\u003chr/\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\nBecause you get instant feedback, you should feel encouraged to experiment, so do not hesitate to edit the Scala source code and re-execute the cell.",
      "dateUpdated": "Feb 25, 2017 9:41:23 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555192_-612135665",
      "id": "20161205-143112_1129105608",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eThe Spark/Scala interpreter\u003c/h3\u003e\n\u003cp\u003eLet\u0027s dive into the Spark/Scala interpreter now. First, we need to describe the Scala language a bit more.\u003c/p\u003e\n\u003cp\u003eScala is a functional object-oriented language.\u003c/p\u003e\n\u003cp\u003eThe following row shows some commands in Scala. Do note that you can use autocompletion with \u003ccode\u003eCtrl + .\u003c/code\u003e\u003c/p\u003e\n\u003chr/\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\n\u003cp\u003eBecause you get instant feedback, you should feel encouraged to experiment, so do not hesitate to edit the Scala source code and re-execute the cell.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:25 PM",
      "dateFinished": "Feb 25, 2017 9:41:25 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Some arithmetic",
      "text": "8 * 5 + 8 //modifié",
      "dateUpdated": "Feb 25, 2017 10:12:07 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 4.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555192_-612135665",
      "id": "20161215-101647_1780031444",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres67: Int \u003d 48\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 10:03:17 PM",
      "dateFinished": "Feb 25, 2017 10:03:18 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Declaring variables",
      "text": "val a \u003d 6 // val defines an immutable value \u003d\u003d\u003e modifié\nvar b : Int \u003d 5 // var defines a mutable variable\nprintln(a + b)\n\nb \u003d 42\nprintln(a + b)\n\n// Note that the type of a variable or function is written after the variable or function. \n\n// Because Scala does type inference, it is also not necessary to declare type to variables when it looks obvious.",
      "dateUpdated": "Feb 25, 2017 10:13:23 PM",
      "config": {
        "enabled": true,
        "title": true,
        "tableHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 4.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555200_-529029903",
      "id": "20161215-101655_1153115867",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\na: Int \u003d 6\n\nb: Int \u003d 5\n11\n\nb: Int \u003d 42\n48\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 10:13:23 PM",
      "dateFinished": "Feb 25, 2017 10:13:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Reusing a variable",
      "text": "// variables are accessible accross cells\n\na / b",
      "dateUpdated": "Feb 25, 2017 10:13:32 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 4.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555200_-529029903",
      "id": "20161215-101705_1808342927",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres79: Int \u003d 0\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 10:13:32 PM",
      "dateFinished": "Feb 25, 2017 10:13:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nIn Scala, you are encouraged to use a `val` whenever you can so to avoid mutable code. Scala prefers immutability by design and helps us reason through code only. \n\nIt also provides is with the necessary tooling to reason with immutability in mind. In the following row we study some basic collection methods which makes useless the need for mutable lists and for loops",
      "dateUpdated": "Feb 25, 2017 9:41:27 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555201_-529414652",
      "id": "20161215-103526_1401793073",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eIn Scala, you are encouraged to use a \u003ccode\u003eval\u003c/code\u003e whenever you can so to avoid mutable code. Scala prefers immutability by design and helps us reason through code only.\u003c/p\u003e\n\u003cp\u003eIt also provides is with the necessary tooling to reason with immutability in mind. In the following row we study some basic collection methods which makes useless the need for mutable lists and for loops\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:27 PM",
      "dateFinished": "Feb 25, 2017 9:41:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create immutable sequence of numbers",
      "text": "val sequenceNumbers \u003d 1 to 100",
      "dateUpdated": "Feb 25, 2017 9:41:27 PM",
      "config": {
        "enabled": true,
        "title": true,
        "tableHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555201_-529414652",
      "id": "20161215-104816_1792183840",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nsequenceNumbers: scala.collection.immutable.Range.Inclusive \u003d Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:26 PM",
      "dateFinished": "Feb 25, 2017 9:42:29 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "New list with all elements doubled",
      "text": "sequenceNumbers.map(x \u003d\u003e x * 2)\n\nsequenceNumbers.map(_ * 2)",
      "dateUpdated": "Feb 25, 2017 9:41:28 PM",
      "config": {
        "enabled": true,
        "title": true,
        "tableHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 4.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555202_-528260405",
      "id": "20161215-104825_1497989654",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres11: scala.collection.immutable.IndexedSeq[Int] \u003d Vector(2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200)\n\nres12: scala.collection.immutable.IndexedSeq[Int] \u003d Vector(2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126, 128, 130, 132, 134, 136, 138, 140, 142, 144, 146, 148, 150, 152, 154, 156, 158, 160, 162, 164, 166, 168, 170, 172, 174, 176, 178, 180, 182, 184, 186, 188, 190, 192, 194, 196, 198, 200)\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:27 PM",
      "dateFinished": "Feb 25, 2017 9:42:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "New list with pair numbers removed",
      "text": "sequenceNumbers.filter(x \u003d\u003e x % 2 !\u003d 0)\n\nsequenceNumbers.filter(_ % 2 !\u003d 0)",
      "dateUpdated": "Feb 25, 2017 9:41:28 PM",
      "config": {
        "enabled": true,
        "title": true,
        "tableHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 4.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555202_-528260405",
      "id": "20161215-104829_934946861",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres14: scala.collection.immutable.IndexedSeq[Int] \u003d Vector(1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99)\n\nres15: scala.collection.immutable.IndexedSeq[Int] \u003d Vector(1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99)\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:30 PM",
      "dateFinished": "Feb 25, 2017 9:42:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Sum all elements",
      "text": "sequenceNumbers.reduce((x, y) \u003d\u003e x + y)\n\nsequenceNumbers.reduce(_ + _)",
      "dateUpdated": "Feb 25, 2017 9:41:29 PM",
      "config": {
        "enabled": true,
        "title": true,
        "tableHide": true,
        "graph": {
          "mode": "table",
          "height": 86.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 4.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555202_-528260405",
      "id": "20161215-105147_1170476157",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres17: Int \u003d 5050\n\nres18: Int \u003d 5050\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:35 PM",
      "dateFinished": "Feb 25, 2017 9:42:41 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n###Primer on functional programming\nBefore going further, we need to go back to functional programming (sometimes abbreviated to FP in literature).\n\nIn functional programming, we construct our programs using _pure functions_, that is a function that takes an input and produces an output without any side-effects, thus forbidding mutation.\n\nAn essential and powerful concept of FP is the ability to pass functions as arguments. For example in the row below :",
      "dateUpdated": "Feb 25, 2017 9:41:29 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555203_-528645154",
      "id": "20161215-105827_1534509278",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003ePrimer on functional programming\u003c/h3\u003e\n\u003cp\u003eBefore going further, we need to go back to functional programming (sometimes abbreviated to FP in literature).\u003c/p\u003e\n\u003cp\u003eIn functional programming, we construct our programs using \u003cem\u003epure functions\u003c/em\u003e, that is a function that takes an input and produces an output without any side-effects, thus forbidding mutation.\u003c/p\u003e\n\u003cp\u003eAn essential and powerful concept of FP is the ability to pass functions as arguments. For example in the row below :\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:30 PM",
      "dateFinished": "Feb 25, 2017 9:41:30 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "/*\n * The following function adds 2 to a number.\n */\ndef add2(x: Int): Int \u003d x + 2",
      "dateUpdated": "Feb 25, 2017 9:41:30 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 4.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555203_-528645154",
      "id": "20161215-142142_191089995",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nadd2: (x: Int)Int\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:38 PM",
      "dateFinished": "Feb 25, 2017 9:42:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "/* now let\u0027s pass add2 function \n * to map,\n * a function that applies a function to each element of a  * list\n */\n\nList(1,2,3).map(add2)\n",
      "dateUpdated": "Feb 25, 2017 9:41:31 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 86.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 4.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555204_-530568899",
      "id": "20161215-110440_1947087088",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres23: List[Int] \u003d List(3, 4, 5)\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:41 PM",
      "dateFinished": "Feb 25, 2017 9:42:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// in the following we pass an anonymous lambda function \n// in the filter function\nList(1,2,3,4).filter(x \u003d\u003e x \u003e 2)\nList(1,2,3,4).filter(_ \u003e 2)",
      "dateUpdated": "Feb 25, 2017 9:41:31 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 86.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 4.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555204_-530568899",
      "id": "20161215-110528_1134018400",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres26: List[Int] \u003d List(3, 4)\n\nres27: List[Int] \u003d List(3, 4)\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:42 PM",
      "dateFinished": "Feb 25, 2017 9:42:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nHere we have presented the first two higher order functions we are going to use in this course : `map` and `filter`. Those are called _element-wise_ transformations in that they take in a function and applies it to each element in the collection.\n\nWe can use them to do a number of things, from fetching the website associated with each URL in our collection to just squaring numbers. Imagine this scenario :\n\n```scala\n/*\n * Parses a file with URLs and puts them into a list of Strings, each being an URL to get\n */\ndef loadURLs(path: String): List[String] \u003d {\n    ...some code here to return list of urls...\n}\n\n/*\n * GET request on an URL and return the body\n */\ndef getBody(url: String): String \u003d {\n    ...some code here to return body...\n}\n\n/*\n * Detect if body consists of an image, we don\u0027t want those\n */\ndef filterImage(body: String): Boolean \u003d {\n    ...some code here to detect if it\u0027s an image...\n}\n\n// code to load a list of URLs and only return those who are not images\nval allURLsWithoutImages \u003d loadURLs(\"/data/list_of_urls.csv\").map(getBody).filter(filterImage)\n\n```",
      "dateUpdated": "Feb 25, 2017 9:41:31 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555205_-530953648",
      "id": "20170125-085058_1374080418",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eHere we have presented the first two higher order functions we are going to use in this course : \u003ccode\u003emap\u003c/code\u003e and \u003ccode\u003efilter\u003c/code\u003e. Those are called \u003cem\u003eelement-wise\u003c/em\u003e transformations in that they take in a function and applies it to each element in the collection.\u003c/p\u003e\n\u003cp\u003eWe can use them to do a number of things, from fetching the website associated with each URL in our collection to just squaring numbers. Imagine this scenario :\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003e/*\n * Parses a file with URLs and puts them into a list of Strings, each being an URL to get\n */\ndef loadURLs(path: String): List[String] \u003d {\n    ...some code here to return list of urls...\n}\n\n/*\n * GET request on an URL and return the body\n */\ndef getBody(url: String): String \u003d {\n    ...some code here to return body...\n}\n\n/*\n * Detect if body consists of an image, we don\u0027t want those\n */\ndef filterImage(body: String): Boolean \u003d {\n    ...some code here to detect if it\u0027s an image...\n}\n\n// code to load a list of URLs and only return those who are not images\nval allURLsWithoutImages \u003d loadURLs(\"/data/list_of_urls.csv\").map(getBody).filter(filterImage)\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:32 PM",
      "dateFinished": "Feb 25, 2017 9:41:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \nThe other essential higher order function you need to learn is `reduce`, which walks a function through all the elements in a collection to produce an aggregated result. \n\nTo do that, the function operates on two elements in the sequence, and returns a new element of the same type, which is then passed to the same function with the next element in the list.\n\nA simple example of such a function is +, which we can use to sum all of the elements in our list.",
      "dateUpdated": "Feb 25, 2017 9:41:32 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555205_-530953648",
      "id": "20170125-085533_1923530403",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eThe other essential higher order function you need to learn is \u003ccode\u003ereduce\u003c/code\u003e, which walks a function through all the elements in a collection to produce an aggregated result.\u003c/p\u003e\n\u003cp\u003eTo do that, the function operates on two elements in the sequence, and returns a new element of the same type, which is then passed to the same function with the next element in the list.\u003c/p\u003e\n\u003cp\u003eA simple example of such a function is +, which we can use to sum all of the elements in our list.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:32 PM",
      "dateFinished": "Feb 25, 2017 9:41:32 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "(1 to 1000).reduce((x,y) \u003d\u003e x+y)\n(1 to 1000).reduce(_+_)",
      "dateUpdated": "Feb 25, 2017 9:41:32 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555205_-530953648",
      "id": "20170125-085545_856912582",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres29: Int \u003d 500500\n\nres30: Int \u003d 500500\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:44 PM",
      "dateFinished": "Feb 25, 2017 9:42:48 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\nIn the previous row, imagine that the sum function operates on 1 and 2 and returns 3, \n\nthen that result is passed to the function along with the next element which is 3, and the result 6 is passed with the next element 4, and then the result 10 is passed with 5 etc...\n\n1 --|+\n2 --| 3 --|+\n3 --------| 6 --| +\n4 --------------| 10 --| +\n5 ---------------------| 15\n.etc...\n\nSimilar to `reduce` is `fold`, which takes a \"zero value\" in addition and passed in the initial call",
      "dateUpdated": "Feb 25, 2017 9:41:33 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555206_-529799401",
      "id": "20170125-085558_1894989290",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eIn the previous row, imagine that the sum function operates on 1 and 2 and returns 3,\u003c/p\u003e\n\u003cp\u003ethen that result is passed to the function along with the next element which is 3, and the result 6 is passed with the next element 4, and then the result 10 is passed with 5 etc\u0026hellip;\u003c/p\u003e\n\u003cp\u003e1 \u0026ndash;|+\n\u003cbr  /\u003e2 \u0026ndash;| 3 \u0026ndash;|+\n\u003cbr  /\u003e3 \u0026mdash;\u0026mdash;\u0026ndash;| 6 \u0026ndash;| +\n\u003cbr  /\u003e4 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;| 10 \u0026ndash;| +\n\u003cbr  /\u003e5 \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;| 15\n\u003cbr  /\u003e.etc\u0026hellip;\u003c/p\u003e\n\u003cp\u003eSimilar to \u003ccode\u003ereduce\u003c/code\u003e is \u003ccode\u003efold\u003c/code\u003e, which takes a \u0026ldquo;zero value\u0026rdquo; in addition and passed in the initial call\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:33 PM",
      "dateFinished": "Feb 25, 2017 9:41:33 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\nTry to solve the following rows. By editing the `val output \u003d ...` line, use `map`, `filter` and `reduce` with anonymous functions on the variable `input` to return a variable `output` that will be compared to the `expected`variable.\n\nRemember your session is interactive, it makes it easy to test intermediary values when chaining functions, example :\n```scala\ninput.map(...)                                  // 1st execution\ninput.map(...).filter(...)                      // 2nd execution\ninput.map(...).filter(...).reduce(...)          // 3rd execution\n```",
      "dateUpdated": "Feb 25, 2017 9:41:33 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555206_-529799401",
      "id": "20170125-085605_508597900",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\n\u003cp\u003eTry to solve the following rows. By editing the \u003ccode\u003eval output \u003d ...\u003c/code\u003e line, use \u003ccode\u003emap\u003c/code\u003e, \u003ccode\u003efilter\u003c/code\u003e and \u003ccode\u003ereduce\u003c/code\u003e with anonymous functions on the variable \u003ccode\u003einput\u003c/code\u003e to return a variable \u003ccode\u003eoutput\u003c/code\u003e that will be compared to the \u003ccode\u003eexpected\u003c/code\u003evariable.\u003c/p\u003e\n\u003cp\u003eRemember your session is interactive, it makes it easy to test intermediary values when chaining functions, example :\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003einput.map(...)                                  // 1st execution\ninput.map(...).filter(...)                      // 2nd execution\ninput.map(...).filter(...).reduce(...)          // 3rd execution\n\u003c/code\u003e\u003c/pre\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:34 PM",
      "dateFinished": "Feb 25, 2017 9:41:34 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Return the list of squared numbers from 1 to 5",
      "text": "val input \u003d List(1, 2, 3, 4, 5)\nval output \u003d input.map(x \u003d\u003e x)\n\nval expected \u003d List(1, 4, 9, 16, 25) ",
      "dateUpdated": "Feb 25, 2017 9:41:34 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555207_-530184150",
      "id": "20170125-085713_1325612841",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\ninput: List[Int] \u003d List(1, 2, 3, 4, 5)\n\noutput: List[Int] \u003d List(1, 2, 3, 4, 5)\n\nexpected: List[Int] \u003d List(1, 4, 9, 16, 25)\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:46 PM",
      "dateFinished": "Feb 25, 2017 9:42:51 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Return the sum of elements from 1 to 5",
      "text": "val input \u003d List(1, 2, 3, 4, 5)\nval output \u003d input.reduce((x, y) \u003d\u003e x)\n\nval expected \u003d 15",
      "dateUpdated": "Feb 25, 2017 9:41:34 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555207_-530184150",
      "id": "20170125-085720_2133991651",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\ninput: List[Int] \u003d List(1, 2, 3, 4, 5)\n\noutput: Int \u003d 1\n\nexpected: Int \u003d 15\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:49 PM",
      "dateFinished": "Feb 25, 2017 9:42:53 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Return the sum of squared pair numbers between 1 and 100",
      "text": "val input \u003d 1 to 100\nval output \u003d input.reduce((x, y) \u003d\u003e x)\n\nval expected \u003d 171700",
      "dateUpdated": "Feb 25, 2017 9:41:34 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555208_-532107894",
      "id": "20170125-085727_408760662",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\ninput: scala.collection.immutable.Range.Inclusive \u003d Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100)\n\noutput: Int \u003d 1\n\nexpected: Int \u003d 171700\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:51 PM",
      "dateFinished": "Feb 25, 2017 9:42:55 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Get highest number in a list with a reduce",
      "text": "import util.Random.nextInt\nval input \u003d Seq.fill(10)(nextInt)\nval output \u003d input.reduce((x, y) \u003d\u003e x)\n\nval expected \u003d input.max",
      "dateUpdated": "Feb 25, 2017 9:41:35 PM",
      "config": {
        "enabled": true,
        "title": true,
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 6.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555209_-532492643",
      "id": "20170125-085732_1286059698",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport util.Random.nextInt\n\ninput: Seq[Int] \u003d List(-1115347586, -75625225, -439501623, -1095250297, 503305667, 1359356791, -1196777520, -850727794, 1547079040, 661359403)\n\noutput: Int \u003d -1115347586\n\nexpected: Int \u003d 1547079040\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:54 PM",
      "dateFinished": "Feb 25, 2017 9:42:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n### Programming with RDDs\nIn this chapter, we are going to introduce Spark\u0027s core abstraction for working with data in a distributed and resilient way : the \u003ctext style\u003d\"color:red;\"\u003eresilient distributed dataset\u003c/text\u003e, or \u003ctext style\u003d\"color:red;\"\u003eRDD\u003c/text\u003e. Under the hood, Spark automatically performs the distribution of RDDs and its processing around the cluster, so we can focus on our code and not on distributed processing problems, such as the handling of data locality or resiliency in case of node failure.\n\nA RDD consists of a collection of elements partitioned accross the nodes of a cluster of machines that can be operated on in parallel. In Spark, work is expressed by the creation and transformation of RDDs using Spark operators.\n\n\u003ctext style\u003d\"color:red;\"\u003eNote\u003c/text\u003e : RDD is the core data structure to Spark, but the style of programming we are studying in this lesson is considered the _lowest-level API_ for Spark. The Spark community is pushing the use of Structured programming with Dataframes/Datasets instead, an optimized interface for working with structured and semi-structured data, which we will learn later. Understanding RDDs is still important because it teaches you how Spark works under the hood and will serve you to understand and optimize your application when deployed into production.\n\nThis example displays the coarse-grained processing nature of Spark, that applies the same operation to many data items. This is a good fit for many parallel applications, as they _naturally apply the same operation to multiple data items_.",
      "dateUpdated": "Feb 25, 2017 9:41:35 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555209_-532492643",
      "id": "20170125-092502_905179202",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eProgramming with RDDs\u003c/h3\u003e\n\u003cp\u003eIn this chapter, we are going to introduce Spark\u0027s core abstraction for working with data in a distributed and resilient way : the \u003ctext style\u003d\"color:red;\"\u003eresilient distributed dataset\u003c/text\u003e, or \u003ctext style\u003d\"color:red;\"\u003eRDD\u003c/text\u003e. Under the hood, Spark automatically performs the distribution of RDDs and its processing around the cluster, so we can focus on our code and not on distributed processing problems, such as the handling of data locality or resiliency in case of node failure.\u003c/p\u003e\n\u003cp\u003eA RDD consists of a collection of elements partitioned accross the nodes of a cluster of machines that can be operated on in parallel. In Spark, work is expressed by the creation and transformation of RDDs using Spark operators.\u003c/p\u003e\n\u003cp\u003e\u003ctext style\u003d\"color:red;\"\u003eNote\u003c/text\u003e : RDD is the core data structure to Spark, but the style of programming we are studying in this lesson is considered the \u003cem\u003elowest-level API\u003c/em\u003e for Spark. The Spark community is pushing the use of Structured programming with Dataframes/Datasets instead, an optimized interface for working with structured and semi-structured data, which we will learn later. Understanding RDDs is still important because it teaches you how Spark works under the hood and will serve you to understand and optimize your application when deployed into production.\u003c/p\u003e\n\u003cp\u003eThis example displays the coarse-grained processing nature of Spark, that applies the same operation to many data items. This is a good fit for many parallel applications, as they \u003cem\u003enaturally apply the same operation to multiple data items\u003c/em\u003e.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:35 PM",
      "dateFinished": "Feb 25, 2017 9:41:35 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\r\n### Running **Spark**\r\nThe variable **sc** allows you to access a Spark Context to run your Spark programs.\r\n* For more information about Spark, please refer to [Spark Overview](https://spark.apache.org/docs/latest/)\r\n\r\n**Important note:** Do not create the *sc* variable - it is already initialized for you.",
      "dateUpdated": "Feb 25, 2017 9:41:35 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555210_-531338396",
      "id": "20170125-093118_646587966",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eRunning \u003cstrong\u003eSpark\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThe variable \u003cstrong\u003esc\u003c/strong\u003e allows you to access a Spark Context to run your Spark programs.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFor more information about Spark, please refer to \u003ca href\u003d\"https://spark.apache.org/docs/latest/\"\u003eSpark Overview\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eImportant note:\u003c/strong\u003e Do not create the \u003cem\u003esc\u003c/em\u003e variable - it is already initialized for you.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:36 PM",
      "dateFinished": "Feb 25, 2017 9:41:36 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// A SparkContext is already created for you by Zeppelin.\r\n// Do not create another or unspecified behavior may occur.\r\nsc.version\r\n\r\n// create an RDD from a local collection with parallelize and then sum all numbers in rdd.\r\nsc.parallelize(1 to 100000000).map(x \u003d\u003e x^2).reduce((x,y) \u003d\u003e x + y)",
      "dateUpdated": "Feb 25, 2017 9:41:36 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555210_-531338396",
      "id": "20170125-093207_1644704199",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nres37: String \u003d 2.0.2\n\nres38: Int \u003d 987459712\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:55 PM",
      "dateFinished": "Feb 25, 2017 9:43:11 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nAs you can see, the RDD API is very similar to the functions we have previously used for processing entire collections. \n\nIf you already are a Scala developer, or are used to functional programming on collections, then RDDs act like a collection that Spark parallelizes on the cluster under the hood.\n\nSpark also provides us with more generic functions.\n\u003chr/\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\nTry to solve the following row. Browse the Spark API documentation to find the corresponding function.",
      "dateUpdated": "Feb 25, 2017 9:41:37 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555212_-533646890",
      "id": "20170125-094956_997092478",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eAs you can see, the RDD API is very similar to the functions we have previously used for processing entire collections.\u003c/p\u003e\n\u003cp\u003eIf you already are a Scala developer, or are used to functional programming on collections, then RDDs act like a collection that Spark parallelizes on the cluster under the hood.\u003c/p\u003e\n\u003cp\u003eSpark also provides us with more generic functions.\u003c/p\u003e\n\u003chr/\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\n\u003cp\u003eTry to solve the following row. Browse the Spark API documentation to find the corresponding function.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:37 PM",
      "dateFinished": "Feb 25, 2017 9:41:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Calculate the number of unique words in the \"words\" RDD here.",
      "text": "val words \u003d sc.parallelize(Array(\"hello\", \"world\", \"goodbye\", \"hello\", \"again\"))\n\nval expected \u003d 4",
      "dateUpdated": "Feb 25, 2017 9:41:37 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555212_-533646890",
      "id": "20170125-093416_1662109555",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nwords: org.apache.spark.rdd.RDD[String] \u003d ParallelCollectionRDD[2] at parallelize at \u003cconsole\u003e:28\n\nexpected: Int \u003d 4\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:42:59 PM",
      "dateFinished": "Feb 25, 2017 9:43:13 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Create a random RDD of numbers, and find its mean.",
      "text": "import util.Random.nextInt\nval input \u003d sc.parallelize(Seq.fill(10000)(nextInt))",
      "dateUpdated": "Feb 25, 2017 9:41:38 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555213_-534031639",
      "id": "20170125-093437_413700163",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nimport util.Random.nextInt\n\ninput: org.apache.spark.rdd.RDD[Int] \u003d ParallelCollectionRDD[3] at parallelize at \u003cconsole\u003e:30\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:43:11 PM",
      "dateFinished": "Feb 25, 2017 9:43:15 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n#### Key value pairs\n\nWhen doing Hadoop MapReduce, you are used to working with key-value pairs. \n\nIn Scala, we use tuples to manage multiple data altogether.\n\n```scala\nval t \u003d (1, \"hello\", Console)\n```\n\nIt is possible to access elements in a tuple :\n\n```scala\nval t \u003d (4,3,2,1)\nval sum \u003d t._1 + t._2 + t._3 + t._4\n```\n\nNow remember the reduce example, which takes 2 elements organized as tuples :\n\n```scala\n(1 to 1000).reduce((x,y) \u003d\u003e x+y)\n```\n\nYou can deconstruct the tuple inside the anonymous function, it makes it easier to reason around them. This can make for some convoluted one liners.\n\n```scala\n(1 to 1000)\n    .map(x \u003d\u003e (x, 2*x, 3*x))\n    .reduce{case ((x1, doubleX1, tripleX1), (x2, doubleX2, tripleX2)) \u003d\u003e (x1 + x2, doubleX1 * doubleX2, tripleX1 - tripleX2)}\n```\n\n\u003chr/\u003e\n\nIn Spark, a RDD of tuples of 2 elements is considered a key-value RDD. This gives us access to [a new class of functions](https://spark.apache.org/docs/latest/programming-guide.html#working-with-key-value-pairs).\n\nFor example, the following code uses the reduceByKey operation on key-value pairs to count how many times each line of text occurs in a file. It acts like executing reduce on the array of values under the same key.\n\n```scala\nval lines \u003d sc.textFile(\"data.txt\")\nval pairs \u003d lines.map(s \u003d\u003e (s, 1))\nval counts \u003d pairs.reduceByKey((a, b) \u003d\u003e a + b)\n```\n\n\u003chr/\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\n\nTry to solve the following cells.",
      "dateUpdated": "Feb 25, 2017 9:41:38 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {},
          "map": {
            "baseMapType": "Streets",
            "isOnline": true,
            "pinCols": []
          }
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555213_-534031639",
      "id": "20161205-143100_327623105",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eKey value pairs\u003c/h4\u003e\n\u003cp\u003eWhen doing Hadoop MapReduce, you are used to working with key-value pairs.\u003c/p\u003e\n\u003cp\u003eIn Scala, we use tuples to manage multiple data altogether.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003eval t \u003d (1, \"hello\", Console)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIt is possible to access elements in a tuple :\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003eval t \u003d (4,3,2,1)\nval sum \u003d t._1 + t._2 + t._3 + t._4\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNow remember the reduce example, which takes 2 elements organized as tuples :\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003e(1 to 1000).reduce((x,y) \u003d\u0026gt; x+y)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can deconstruct the tuple inside the anonymous function, it makes it easier to reason around them. This can make for some convoluted one liners.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003e(1 to 1000)\n    .map(x \u003d\u0026gt; (x, 2*x, 3*x))\n    .reduce{case ((x1, doubleX1, tripleX1), (x2, doubleX2, tripleX2)) \u003d\u0026gt; (x1 + x2, doubleX1 * doubleX2, tripleX1 - tripleX2)}\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr/\u003e\n\u003cp\u003eIn Spark, a RDD of tuples of 2 elements is considered a key-value RDD. This gives us access to \u003ca href\u003d\"https://spark.apache.org/docs/latest/programming-guide.html#working-with-key-value-pairs\"\u003ea new class of functions\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor example, the following code uses the reduceByKey operation on key-value pairs to count how many times each line of text occurs in a file. It acts like executing reduce on the array of values under the same key.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003eval lines \u003d sc.textFile(\"data.txt\")\nval pairs \u003d lines.map(s \u003d\u0026gt; (s, 1))\nval counts \u003d pairs.reduceByKey((a, b) \u003d\u0026gt; a + b)\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr/\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\n\u003cp\u003eTry to solve the following cells.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:39 PM",
      "dateFinished": "Feb 25, 2017 9:41:39 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Wordcount, easy mode",
      "text": "val input \u003d sc.parallelize(Seq(\"hi\", \"my\", \"my\", \"name\", \"is\",\"hi\"))\nval output \u003d input\n\nval expected \u003d Array((\"is\",1), (\"my,2\"), (\"name\",1), (\"hi\",2))",
      "dateUpdated": "Feb 25, 2017 9:41:39 PM",
      "config": {
        "enabled": true,
        "title": true,
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555215_-533262141",
      "id": "20170125-095836_160099750",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\ninput: org.apache.spark.rdd.RDD[String] \u003d ParallelCollectionRDD[4] at parallelize at \u003cconsole\u003e:30\n\noutput: org.apache.spark.rdd.RDD[String] \u003d ParallelCollectionRDD[4] at parallelize at \u003cconsole\u003e:30\n\nexpected: Array[java.io.Serializable] \u003d Array((is,1), my,2, (name,1), (hi,2))\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:43:13 PM",
      "dateFinished": "Feb 25, 2017 9:43:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Sum ages per gender",
      "text": "val rdd1 \u003d sc.parallelize(Seq((\"Patrick\", \"homme\"), (\"Sandra\", \"femme\"), (\"Faniki\", \"homme\"), (\"Noemie\", \"femme\"), (\"Francois\", \"homme\"), (\"Cassandre\", \"femme\")))\nval rdd2 \u003d sc.parallelize(Seq((\"Patrick\", 20), (\"Sandra\", 25), (\"Faniki\", 20), (\"Noemie\", 20), (\"Francois\", 30), (\"Cassandre\", 18)))\n\n// rdd1 and rdd2 are key-value rdds so you can join them and then continue the computation\nval output \u003d rdd1.join(rdd2)\n\nval expected \u003d Array((\"femme\", 63), (\"homme\", 70))",
      "dateUpdated": "Feb 25, 2017 9:41:40 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555215_-533262141",
      "id": "20170125-130411_450751393",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nrdd1: org.apache.spark.rdd.RDD[(String, String)] \u003d ParallelCollectionRDD[5] at parallelize at \u003cconsole\u003e:30\n\nrdd2: org.apache.spark.rdd.RDD[(String, Int)] \u003d ParallelCollectionRDD[6] at parallelize at \u003cconsole\u003e:30\n\noutput: org.apache.spark.rdd.RDD[(String, (String, Int))] \u003d MapPartitionsRDD[9] at join at \u003cconsole\u003e:34\n\nexpected: Array[(String, Int)] \u003d Array((femme,63), (homme,70))\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:43:15 PM",
      "dateFinished": "Feb 25, 2017 9:43:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md \n\n#### To the real wordcount\n\nWe are missing an important operator before going to the normal wordcount as we know it : `flatten`, which flattens the lists held inside the outer list into one resulting list.\n\n```scala\nscala\u003e val lol \u003d List(List(1,2), List(3,4))\nlol: List[List[Int]] \u003d List(List(1, 2), List(3, 4))\n\nscala\u003e val result \u003d lol.flatten\nresult: List[Int] \u003d List(1, 2, 3, 4)\n```\n\nIn general, when reading a text, you get an array of lines like :\n\n```scala\nval text \u003d Seq(\n    \"coucou\",\n    \"ceci est le cours de mapreduce\",\n    \"ce n\u0027est pas difficile\",\n    \"n\u0027est ce pas ?\"\n)\n```\n\nif I want the list of all the words, first I need to split each line into words and then flatten the whole :\n\n```scala\nscala\u003e val split \u003d text.map(line \u003d\u003e line.split(\" \"))\nresult: List[Array(String)] \u003d List(Array(\"coucou\"), Array(\"ceci\", \"est\", \"le\" ...))\n\nscala\u003e val flat \u003d split.flatten\nresult: List[String] \u003d List(\"coucou\", \"ceci\", \"est\", \"le\" ...))\n```\n\nYou get the same result if you use `flatMap` :\n\n```scala\nscala\u003e val flat \u003d text.flatMap(line \u003d\u003e line.split(\" \"))\nresult: List[String] \u003d List(\"coucou\", \"ceci\", \"est\", \"le\" ...))\n```\n\n\u003chr/\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\n\nTry the wordcount on a textfile `/opt/dataset/don-quijote.txt.gz` read with Spark into a RDD ! This is it, this is the classic Hello world, it uses `flatMap` as the Hadoop Map and `reduceByKey` as the Hadoop Reduce.",
      "dateUpdated": "Feb 25, 2017 9:41:40 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555215_-533262141",
      "id": "20170125-125907_1086033982",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003eTo the real wordcount\u003c/h4\u003e\n\u003cp\u003eWe are missing an important operator before going to the normal wordcount as we know it : \u003ccode\u003eflatten\u003c/code\u003e, which flattens the lists held inside the outer list into one resulting list.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003escala\u0026gt; val lol \u003d List(List(1,2), List(3,4))\nlol: List[List[Int]] \u003d List(List(1, 2), List(3, 4))\n\nscala\u0026gt; val result \u003d lol.flatten\nresult: List[Int] \u003d List(1, 2, 3, 4)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIn general, when reading a text, you get an array of lines like :\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003eval text \u003d Seq(\n    \"coucou\",\n    \"ceci est le cours de mapreduce\",\n    \"ce n\u0027est pas difficile\",\n    \"n\u0027est ce pas ?\"\n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eif I want the list of all the words, first I need to split each line into words and then flatten the whole :\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003escala\u0026gt; val split \u003d text.map(line \u003d\u0026gt; line.split(\" \"))\nresult: List[Array(String)] \u003d List(Array(\"coucou\"), Array(\"ceci\", \"est\", \"le\" ...))\n\nscala\u0026gt; val flat \u003d split.flatten\nresult: List[String] \u003d List(\"coucou\", \"ceci\", \"est\", \"le\" ...))\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou get the same result if you use \u003ccode\u003eflatMap\u003c/code\u003e :\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003escala\u0026gt; val flat \u003d text.flatMap(line \u003d\u0026gt; line.split(\" \"))\nresult: List[String] \u003d List(\"coucou\", \"ceci\", \"est\", \"le\" ...))\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr/\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\n\u003cp\u003eTry the wordcount on a textfile \u003ccode\u003e/opt/dataset/don-quijote.txt.gz\u003c/code\u003e read with Spark into a RDD ! This is it, this is the classic Hello world, it uses \u003ccode\u003eflatMap\u003c/code\u003e as the Hadoop Map and \u003ccode\u003ereduceByKey\u003c/code\u003e as the Hadoop Reduce.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:40 PM",
      "dateFinished": "Feb 25, 2017 9:41:40 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// load file into RDD and check 10 first lines\n\nval text \u003d sc.textFile(\"/opt/dataset/don-quijote.txt.gz\")\ntext.take(3)",
      "dateUpdated": "Feb 25, 2017 9:41:40 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555216_-522873921",
      "id": "20170125-134931_765137162",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\ntext: org.apache.spark.rdd.RDD[String] \u003d /opt/dataset/don-quijote.txt.gz MapPartitionsRDD[11] at textFile at \u003cconsole\u003e:32\n\nres45: Array[String] \u003d Array(The Project Gutenberg EBook of Don Quijote, by Miguel de Cervantes Saavedra, \"\", This eBook is for the use of anyone anywhere at no cost and with)\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:43:18 PM",
      "dateFinished": "Feb 25, 2017 9:43:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// write your Spark map-reduce code here\n\ntext.doSomething",
      "dateUpdated": "Feb 25, 2017 9:41:41 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555216_-522873921",
      "id": "20170125-135340_1546349786",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\n\n\n\u003cconsole\u003e:35: error: value doSomething is not a member of org.apache.spark.rdd.RDD[String]\n       text.doSomething\n            ^\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:43:21 PM",
      "dateFinished": "Feb 25, 2017 9:43:23 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\r\n### Working with **Spark SQL and DataFrames**\r\nThe variable **sqlContext** allows you to access a Spark SQL Context to work with Spark SQL and DataFrames.\r\n* Scala can be used to create Spark [DataFrames](http://spark.apache.org/docs/latest/sql-programming-guide.html) - a distributed collection of data organized into named columns.\r\n* DataFrames are created by appending ``.toDF()`` to the Scala RDD\r\n\r\nIn this tutorial, we will analyze the ipligence-lite csv file with SparkSQL. It is a Free IP location database solution to detect the country and continent of the visitor connection based on their IP address. [Download here if needed](http://www.ipligence.com/free-ip-database). You\u0027ll get the schema of the file there.\r\n\r\nBefore, we had to use [spark-csv](https://github.com/databricks/spark-csv) and use reflection to build Dataframes from csv files. Now spark-csv is directly integrated into Spark as a Data Source, so it is easy to parse csv files into Dataframes.\r\n\r\n**Important note:** Do not create the *sqlContext* variable - it is already initialized for you.",
      "dateUpdated": "Feb 25, 2017 9:41:41 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555219_-522489172",
      "id": "20170125-125237_922206168",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch3\u003eWorking with \u003cstrong\u003eSpark SQL and DataFrames\u003c/strong\u003e\u003c/h3\u003e\n\u003cp\u003eThe variable \u003cstrong\u003esqlContext\u003c/strong\u003e allows you to access a Spark SQL Context to work with Spark SQL and DataFrames.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eScala can be used to create Spark \u003ca href\u003d\"http://spark.apache.org/docs/latest/sql-programming-guide.html\"\u003eDataFrames\u003c/a\u003e - a distributed collection of data organized into named columns.\u003c/li\u003e\n\u003cli\u003eDataFrames are created by appending \u003ccode\u003e.toDF()\u003c/code\u003e to the Scala RDD\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn this tutorial, we will analyze the ipligence-lite csv file with SparkSQL. It is a Free IP location database solution to detect the country and continent of the visitor connection based on their IP address. \u003ca href\u003d\"http://www.ipligence.com/free-ip-database\"\u003eDownload here if needed\u003c/a\u003e. You\u0027ll get the schema of the file there.\u003c/p\u003e\n\u003cp\u003eBefore, we had to use \u003ca href\u003d\"https://github.com/databricks/spark-csv\"\u003espark-csv\u003c/a\u003e and use reflection to build Dataframes from csv files. Now spark-csv is directly integrated into Spark as a Data Source, so it is easy to parse csv files into Dataframes.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eImportant note:\u003c/strong\u003e Do not create the \u003cem\u003esqlContext\u003c/em\u003e variable - it is already initialized for you.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:42 PM",
      "dateFinished": "Feb 25, 2017 9:41:42 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// let\u0027s check the file we are going to use\nval ipligence \u003d sc.textFile(\"/opt/dataset/ipligence-lite.csv\")\nipligence.take(1)",
      "dateUpdated": "Feb 25, 2017 9:41:42 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555219_-522489172",
      "id": "20170201-065922_1040411609",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\nipligence: org.apache.spark.rdd.RDD[String] \u003d /opt/dataset/ipligence-lite.csv MapPartitionsRDD[13] at textFile at \u003cconsole\u003e:31\n\nres50: Array[String] \u003d Array(\"0000000000\",\"0016777215\",\"\",\"\",\"\",\"\")\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:43:23 PM",
      "dateFinished": "Feb 25, 2017 9:43:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "// create your dataframe by reading the csv file\nval columnNames \u003d Seq(\"IP_FROM\", \"IP_TO\", \"COUNTRY_CODE\", \"COUNTRY_NAME\", \"CONTINENT_CODE\", \"CONTINENT_NAME\")\nval df \u003d sqlContext.read.csv(\"/opt/dataset/ipligence-lite.csv\").toDF(columnNames: _*)\ndf.printSchema()\ndf.show()\n\n// if you want to use the dataframe in SQL, you must register it\ndf.registerTempTable(\"df\")",
      "dateUpdated": "Feb 25, 2017 9:41:42 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555220_-524412916",
      "id": "20170201-071617_1372276620",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "\ncolumnNames: Seq[String] \u003d List(IP_FROM, IP_TO, COUNTRY_CODE, COUNTRY_NAME, CONTINENT_CODE, CONTINENT_NAME)\n\ndf: org.apache.spark.sql.DataFrame \u003d [IP_FROM: string, IP_TO: string ... 4 more fields]\nroot\n |-- IP_FROM: string (nullable \u003d true)\n |-- IP_TO: string (nullable \u003d true)\n |-- COUNTRY_CODE: string (nullable \u003d true)\n |-- COUNTRY_NAME: string (nullable \u003d true)\n |-- CONTINENT_CODE: string (nullable \u003d true)\n |-- CONTINENT_NAME: string (nullable \u003d true)\n\n+----------+----------+------------+------------+--------------+--------------+\n|   IP_FROM|     IP_TO|COUNTRY_CODE|COUNTRY_NAME|CONTINENT_CODE|CONTINENT_NAME|\n+----------+----------+------------+------------+--------------+--------------+\n|0000000000|0016777215|        null|        null|          null|          null|\n|0016777216|0016777471|          AU|   AUSTRALIA|            OC|       OCEANIA|\n|0016777472|0016778239|          CN|       CHINA|            AS|          ASIA|\n|0016778240|0016779263|          AU|   AUSTRALIA|            OC|       OCEANIA|\n|0016779264|0016781311|          CN|       CHINA|            AS|          ASIA|\n|0016781312|0016785407|          JP|       JAPAN|            AS|          ASIA|\n|0016785408|0016793599|          CN|       CHINA|            AS|          ASIA|\n|0016793600|0016809983|          JP|       JAPAN|            AS|          ASIA|\n|0016809984|0016842751|          TH|    THAILAND|            AS|          ASIA|\n|0016842752|0016843007|          CN|       CHINA|            AS|          ASIA|\n|0016843008|0016843263|          AU|   AUSTRALIA|            OC|       OCEANIA|\n|0016843264|0016859135|          CN|       CHINA|            AS|          ASIA|\n|0016859136|0016875519|          JP|       JAPAN|            AS|          ASIA|\n|0016875520|0016908287|          TH|    THAILAND|            AS|          ASIA|\n|0016908288|0016909055|          CN|       CHINA|            AS|          ASIA|\n|0016909056|0016909311|          AU|   AUSTRALIA|            OC|       OCEANIA|\n|0016909312|0016941055|          CN|       CHINA|            AS|          ASIA|\n|0016941056|0016973823|          TH|    THAILAND|            AS|          ASIA|\n|0016973824|0017039359|          CN|       CHINA|            AS|          ASIA|\n|0017039360|0017039615|          AU|   AUSTRALIA|            OC|       OCEANIA|\n+----------+----------+------------+------------+--------------+--------------+\nonly showing top 20 rows\n\n\nwarning: there was one deprecation warning; re-run with -deprecation for details\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:43:23 PM",
      "dateFinished": "Feb 25, 2017 9:43:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nYou have two ways to use the dataframe in SparkSQL.\n\n1. From the SQL API of sqlContext : `sqlContext.sql(\"SELECT IP_TO FROM df\")` see this\n2. FROM the functional API : `df.select(\"IP_TO\")`\n\nTo show the beginning of the actual dataframe, use the show() function, which is an action and so executes all of the computation.",
      "dateUpdated": "Feb 25, 2017 9:41:42 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555220_-524412916",
      "id": "20170201-092149_1555341566",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eYou have two ways to use the dataframe in SparkSQL.\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eFrom the SQL API of sqlContext : \u003ccode\u003esqlContext.sql(\"SELECT IP_TO FROM df\")\u003c/code\u003e see this\u003c/li\u003e\n\u003cli\u003eFROM the functional API : \u003ccode\u003edf.select(\"IP_TO\")\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eTo show the beginning of the actual dataframe, use the show() function, which is an action and so executes all of the computation.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:43 PM",
      "dateFinished": "Feb 25, 2017 9:41:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "sqlContext.sql(\"SELECT IP_TO FROM df\").show(5)",
      "dateUpdated": "Feb 25, 2017 9:41:43 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555221_-524797665",
      "id": "20170201-092137_536797865",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----------+\n|     IP_TO|\n+----------+\n|0016777215|\n|0016777471|\n|0016778239|\n|0016779263|\n|0016781311|\n+----------+\nonly showing top 5 rows\n\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:43:25 PM",
      "dateFinished": "Feb 25, 2017 9:43:56 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "df.select(\"IP_TO\").show(5)",
      "dateUpdated": "Feb 25, 2017 9:41:43 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555221_-524797665",
      "id": "20170201-092142_935429249",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "+----------+\n|     IP_TO|\n+----------+\n|0016777215|\n|0016777471|\n|0016778239|\n|0016779263|\n|0016781311|\n+----------+\nonly showing top 5 rows\n\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:43:54 PM",
      "dateFinished": "Feb 25, 2017 9:43:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\nBecause the result of those are Dataframes, you can store them in a new variable or register them for future usage.\n```scala\n// filter with functional api\nval ipDf \u003d sqlContext.sql(\"SELECT IP_FROM,IP_TO FROM df\")\nipDf.filter(\"IP_FROM \u003d \u00270016777216\u0027\").show()\n\n// register from inside SQL API\nsqlContext.sql(\"SELECT IP_FROM,IP_TO,CONTINENT_NAME FROM df WHERE CONTINENT_NAME\u003d\u0027ASIA\u0027\").registerTempTable(\"asiaOnlyDF\")\n```\n\n\u003chr\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\n\nSolve the following queries, using one or both APIs.",
      "dateUpdated": "Feb 25, 2017 9:41:43 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555222_-523643418",
      "id": "20170201-094307_1855187218",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003cp\u003eBecause the result of those are Dataframes, you can store them in a new variable or register them for future usage.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class\u003d\"scala\"\u003e// filter with functional api\nval ipDf \u003d sqlContext.sql(\"SELECT IP_FROM,IP_TO FROM df\")\nipDf.filter(\"IP_FROM \u003d \u00270016777216\u0027\").show()\n\n// register from inside SQL API\nsqlContext.sql(\"SELECT IP_FROM,IP_TO,CONTINENT_NAME FROM df WHERE CONTINENT_NAME\u003d\u0027ASIA\u0027\").registerTempTable(\"asiaOnlyDF\")\n\u003c/code\u003e\u003c/pre\u003e\n\u003chr\u003e\n\u003ch4 style\u003d\"color:red;\"\u003eExercise\u003c/h4\u003e\n\u003cp\u003eSolve the following queries, using one or both APIs.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:43 PM",
      "dateFinished": "Feb 25, 2017 9:41:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Display all continent names",
      "text": "%sql\nselect distinct continent_name from df where continent_name is not null",
      "dateUpdated": "Feb 25, 2017 9:56:24 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "continent_name",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "continent_name",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/sql",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555223_-524028167",
      "id": "20170201-094815_410703547",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "continent_name\nCARIBBEAN\nASIA\nOCEANIA\nNORTH AMERICA\nSOUTH AMERICA\nEUROPE\nANTARCTICA\nAFRICA\nCENTRAL AMERICA\nMIDDLE EAST\nAS\nAF\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:56:24 PM",
      "dateFinished": "Feb 25, 2017 9:56:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Count number of rows with a null value in country code",
      "text": "// Don\u0027t forget you have the count function on a Dataframe after you have filtered it",
      "dateUpdated": "Feb 25, 2017 9:41:44 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555224_-525951912",
      "id": "20170201-094823_1804433902",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:43:58 PM",
      "dateFinished": "Feb 25, 2017 9:43:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Display number of connections per continent",
      "text": "",
      "dateUpdated": "Feb 25, 2017 9:41:44 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555224_-525951912",
      "id": "20170201-100558_849211484",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:43:58 PM",
      "dateFinished": "Feb 25, 2017 9:43:58 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "Show IP_FROM addresses which appear more than once in the file",
      "text": "// PS : if your query is correct, you should only find one",
      "dateUpdated": "Feb 25, 2017 9:41:44 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555225_-526336661",
      "id": "20170201-100608_637001408",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:43:58 PM",
      "dateFinished": "Feb 25, 2017 9:43:59 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%md\n\n#### Primer on Zeppelin visualization\n\nIf your dataframe is registered, you can use the %sql interpreter to directly study it. \n\nNormally the %sql interpreter is used to connect to a Hive metastore, the thing is a registered dataframe is actually present in the local Hive metastore so you can access it as a fake Hive table (you can see it for yourself with `sqlContext.sql(\"SHOW TABLES\").show()`)\n\nWith the %sql interpreter, you have access to basic visualization features of Zeppelin.",
      "dateUpdated": "Feb 25, 2017 9:41:44 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/markdown",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555225_-526336661",
      "id": "20170125-135853_1126980502",
      "result": {
        "code": "SUCCESS",
        "type": "HTML",
        "msg": "\u003ch4\u003ePrimer on Zeppelin visualization\u003c/h4\u003e\n\u003cp\u003eIf your dataframe is registered, you can use the %sql interpreter to directly study it.\u003c/p\u003e\n\u003cp\u003eNormally the %sql interpreter is used to connect to a Hive metastore, the thing is a registered dataframe is actually present in the local Hive metastore so you can access it as a fake Hive table (you can see it for yourself with \u003ccode\u003esqlContext.sql(\"SHOW TABLES\").show()\u003c/code\u003e)\u003c/p\u003e\n\u003cp\u003eWith the %sql interpreter, you have access to basic visualization features of Zeppelin.\u003c/p\u003e\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:41:45 PM",
      "dateFinished": "Feb 25, 2017 9:41:45 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "If your DataFrame is stored internally, you can do some viz",
      "text": "%sql\nSELECT top 100 * FROM df",
      "dateUpdated": "Feb 25, 2017 9:52:52 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": true,
          "keys": [
            {
              "name": "COUNTRY_CODE",
              "index": 2.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "CONTINENT_NAME",
              "index": 5.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "IP_FROM",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "IP_TO",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/sql",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555225_-526336661",
      "id": "20170125-135946_1334292884",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "\nextraneous input \u0027100\u0027 expecting {\u003cEOF\u003e, \u0027(\u0027, \u0027,\u0027, \u0027.\u0027, \u0027[\u0027, \u0027SELECT\u0027, \u0027FROM\u0027, \u0027ADD\u0027, \u0027AS\u0027, \u0027ALL\u0027, \u0027DISTINCT\u0027, \u0027WHERE\u0027, \u0027GROUP\u0027, \u0027BY\u0027, \u0027GROUPING\u0027, \u0027SETS\u0027, \u0027CUBE\u0027, \u0027ROLLUP\u0027, \u0027ORDER\u0027, \u0027HAVING\u0027, \u0027LIMIT\u0027, \u0027AT\u0027, \u0027OR\u0027, \u0027AND\u0027, \u0027IN\u0027, NOT, \u0027NO\u0027, \u0027EXISTS\u0027, \u0027BETWEEN\u0027, \u0027LIKE\u0027, RLIKE, \u0027IS\u0027, \u0027NULL\u0027, \u0027TRUE\u0027, \u0027FALSE\u0027, \u0027NULLS\u0027, \u0027ASC\u0027, \u0027DESC\u0027, \u0027FOR\u0027, \u0027INTERVAL\u0027, \u0027CASE\u0027, \u0027WHEN\u0027, \u0027THEN\u0027, \u0027ELSE\u0027, \u0027END\u0027, \u0027JOIN\u0027, \u0027CROSS\u0027, \u0027OUTER\u0027, \u0027INNER\u0027, \u0027LEFT\u0027, \u0027SEMI\u0027, \u0027RIGHT\u0027, \u0027FULL\u0027, \u0027NATURAL\u0027, \u0027ON\u0027, \u0027LATERAL\u0027, \u0027WINDOW\u0027, \u0027OVER\u0027, \u0027PARTITION\u0027, \u0027RANGE\u0027, \u0027ROWS\u0027, \u0027UNBOUNDED\u0027, \u0027PRECEDING\u0027, \u0027FOLLOWING\u0027, \u0027CURRENT\u0027, \u0027ROW\u0027, \u0027WITH\u0027, \u0027VALUES\u0027, \u0027CREATE\u0027, \u0027TABLE\u0027, \u0027VIEW\u0027, \u0027REPLACE\u0027, \u0027INSERT\u0027, \u0027DELETE\u0027, \u0027INTO\u0027, \u0027DESCRIBE\u0027, \u0027EXPLAIN\u0027, \u0027FORMAT\u0027, \u0027LOGICAL\u0027, \u0027CODEGEN\u0027, \u0027CAST\u0027, \u0027SHOW\u0027, \u0027TABLES\u0027, \u0027COLUMNS\u0027, \u0027COLUMN\u0027, \u0027USE\u0027, \u0027PARTITIONS\u0027, \u0027FUNCTIONS\u0027, \u0027DROP\u0027, \u0027UNION\u0027, \u0027EXCEPT\u0027, \u0027INTERSECT\u0027, \u0027TO\u0027, \u0027TABLESAMPLE\u0027, \u0027STRATIFY\u0027, \u0027ALTER\u0027, \u0027RENAME\u0027, \u0027ARRAY\u0027, \u0027MAP\u0027, \u0027STRUCT\u0027, \u0027COMMENT\u0027, \u0027SET\u0027, \u0027RESET\u0027, \u0027DATA\u0027, \u0027START\u0027, \u0027TRANSACTION\u0027, \u0027COMMIT\u0027, \u0027ROLLBACK\u0027, \u0027MACRO\u0027, \u0027IF\u0027, EQ, \u0027\u003c\u003d\u003e\u0027, \u0027\u003c\u003e\u0027, \u0027!\u003d\u0027, \u0027\u003c\u0027, LTE, \u0027\u003e\u0027, GTE, \u0027+\u0027, \u0027-\u0027, \u0027*\u0027, \u0027/\u0027, \u0027%\u0027, \u0027DIV\u0027, \u0027\u0026\u0027, \u0027|\u0027, \u0027^\u0027, \u0027PERCENT\u0027, \u0027BUCKET\u0027, \u0027OUT\u0027, \u0027OF\u0027, \u0027SORT\u0027, \u0027CLUSTER\u0027, \u0027DISTRIBUTE\u0027, \u0027OVERWRITE\u0027, \u0027TRANSFORM\u0027, \u0027REDUCE\u0027, \u0027USING\u0027, \u0027SERDE\u0027, \u0027SERDEPROPERTIES\u0027, \u0027RECORDREADER\u0027, \u0027RECORDWRITER\u0027, \u0027DELIMITED\u0027, \u0027FIELDS\u0027, \u0027TERMINATED\u0027, \u0027COLLECTION\u0027, \u0027ITEMS\u0027, \u0027KEYS\u0027, \u0027ESCAPED\u0027, \u0027LINES\u0027, \u0027SEPARATED\u0027, \u0027FUNCTION\u0027, \u0027EXTENDED\u0027, \u0027REFRESH\u0027, \u0027CLEAR\u0027, \u0027CACHE\u0027, \u0027UNCACHE\u0027, \u0027LAZY\u0027, \u0027FORMATTED\u0027, TEMPORARY, \u0027OPTIONS\u0027, \u0027UNSET\u0027, \u0027TBLPROPERTIES\u0027, \u0027DBPROPERTIES\u0027, \u0027BUCKETS\u0027, \u0027SKEWED\u0027, \u0027STORED\u0027, \u0027DIRECTORIES\u0027, \u0027LOCATION\u0027, \u0027EXCHANGE\u0027, \u0027ARCHIVE\u0027, \u0027UNARCHIVE\u0027, \u0027FILEFORMAT\u0027, \u0027TOUCH\u0027, \u0027COMPACT\u0027, \u0027CONCATENATE\u0027, \u0027CHANGE\u0027, \u0027CASCADE\u0027, \u0027RESTRICT\u0027, \u0027CLUSTERED\u0027, \u0027SORTED\u0027, \u0027PURGE\u0027, \u0027INPUTFORMAT\u0027, \u0027OUTPUTFORMAT\u0027, DATABASE, DATABASES, \u0027DFS\u0027, \u0027TRUNCATE\u0027, \u0027ANALYZE\u0027, \u0027COMPUTE\u0027, \u0027LIST\u0027, \u0027STATISTICS\u0027, \u0027PARTITIONED\u0027, \u0027EXTERNAL\u0027, \u0027DEFINED\u0027, \u0027REVOKE\u0027, \u0027GRANT\u0027, \u0027LOCK\u0027, \u0027UNLOCK\u0027, \u0027MSCK\u0027, \u0027REPAIR\u0027, \u0027RECOVER\u0027, \u0027EXPORT\u0027, \u0027IMPORT\u0027, \u0027LOAD\u0027, \u0027ROLE\u0027, \u0027ROLES\u0027, \u0027COMPACTIONS\u0027, \u0027PRINCIPALS\u0027, \u0027TRANSACTIONS\u0027, \u0027INDEX\u0027, \u0027INDEXES\u0027, \u0027LOCKS\u0027, \u0027OPTION\u0027, \u0027ANTI\u0027, \u0027LOCAL\u0027, \u0027INPATH\u0027, \u0027CURRENT_DATE\u0027, \u0027CURRENT_TIMESTAMP\u0027, IDENTIFIER, BACKQUOTED_IDENTIFIER}(line 1, pos 11)\n\n\u003d\u003d SQL \u003d\u003d\nSELECT top 100 * FROM df\n-----------^^^\n\nset zeppelin.spark.sql.stacktrace \u003d true to see full stacktrace"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:52:52 PM",
      "dateFinished": "Feb 25, 2017 9:52:52 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "title": "You can visualize the results of a query",
      "text": "%sql\nSELECT CONTINENT_NAME, COUNT(CONTINENT_NAME) as count FROM df GROUP BY CONTINENT_NAME",
      "dateUpdated": "Feb 25, 2017 9:53:18 PM",
      "config": {
        "enabled": true,
        "title": true,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "CONTINENT_NAME",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "CONTINENT_NAME",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "editorMode": "ace/mode/sql",
        "colWidth": 6.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555234_-540572370",
      "id": "20170201-102038_286280778",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "org.apache.spark.SparkException: Job 11 cancelled part of cancelled job group zeppelin-20170201-102038_286280778\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1393)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:795)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:795)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:795)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:795)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1642)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1873)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1899)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)\n\tat org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1935)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1934)\n\tat org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2576)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:1934)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2149)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.zeppelin.spark.ZeppelinContext.showDF(ZeppelinContext.java:216)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:129)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:176)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n"
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:53:18 PM",
      "dateFinished": "Feb 25, 2017 9:53:21 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "",
      "dateUpdated": "Feb 25, 2017 9:41:45 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488057555235_-540957119",
      "id": "20170125-135952_1235249833",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Feb 25, 2017 9:19:15 AM",
      "dateStarted": "Feb 25, 2017 9:44:00 PM",
      "dateFinished": "Feb 25, 2017 9:44:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Practice",
  "id": "2CA8XSX5R",
  "angularObjects": {
    "2C9YR4J7E:shared_process": [],
    "2CA1HJ9V7:shared_process": [],
    "2C9T42U8V:shared_process": [],
    "2CA2N6UNN:shared_process": [],
    "2CBKWNF6D:shared_process": [],
    "2CAR5JPD6:shared_process": [],
    "2CA2T9T2T:shared_process": [],
    "2C9YSVYSE:shared_process": [],
    "2CAQ83RPY:shared_process": [],
    "2CBDE8Z34:shared_process": [],
    "2CA721C6X:shared_process": [],
    "2C9UPEFQM:shared_process": [],
    "2C9A5AZEN:shared_process": [],
    "2CBSNXVYN:shared_process": [],
    "2CCQVE59P:shared_process": [],
    "2CCRT5V74:shared_process": [],
    "2C9YXCC9Q:shared_process": [],
    "2C9XFJ1QK:shared_process": []
  },
  "config": {},
  "info": {}
}